\section {Linear Regression (2.5pts)}
The reading on linear regression located at \url{https://uoft-csc413.github.io/2023/assets/readings/L01a.pdf} may be useful for this question.

Given $n$ pairs of input data with $d$ features and scalar label $(\bx_i,t_i)\in\R^{d}\times\R$, we wish to find a linear model $f(\bx) = \hat{\bw}^\top\bx$ with $\hat{\bw}\in\R^d$ that minimizes the squared error of prediction on the training samples defined below.
This is known as an empirical risk minimizer.
For concise notation, denote the data matrix $X\in\R^{n\times d}$ and the corresponding label vector $\bt\in\R^n$.
The training objective is to minimize the following loss:
\begin{align*}
    \underset{\hat{\bw}}{\mathrm{min}} \,\frac{1}{n}\sum_{i=1}^n (\hat{\bw}^\top\bx_i - t_i)^2 
=   
    \underset{\hat{\bw}}{\mathrm{min}} \,\frac{1}{n}\|X\hat{\bw} - \bt\|_2^2. 
\end{align*}
We assume $X$ is full rank: $X^\top X$ is invertible when $n>d$, and $X X^\top$ is invertible otherwise. Note that when $d>n$, the problem is \textit{underdetermined}, i.e. there are less training samples than parameters to be learned.
This is analogous to learning an \textit{overparameterized} model, which is common when training of deep neural networks.

\subsection{Deriving the Gradient {[0pt]} \LI}\label{sec:a}
Write down the gradient of the loss w.r.t. the learned parameter vector $\hat{\bw}$.

\subsection{Underparameterized Model} \label{sec:b}

\subsubsection{{\color{blue}[0.5pt]} \LI}
First consider the underparameterized $d<n$ case.
Show that the solution obtained by gradient descent is $\hat{\bw} = (X^\top X)^{-1} X^\top\bt$, assuming training converges. Show your work.

\subsubsection{{\color{blue}[0.5pt]} \LII} \label{sec:n_reg}
Now consider the case of noisy linear regression. The training labels $t_i = {\bw^*}^\top\bx_i + \epsilon_i$ are generated by a ground truth linear target function, where the noise term, $\epsilon_i$, is generated independently with zero mean and variance $\sigma^2$. The final training error can be derived as a function of $X$ and $\boldsymbol{\epsilon}$, as:

\begin{align*}
     \textit{Error} = &\frac{1}{n} || (X(X^\top X)^{-1}X^\top - I)\boldsymbol{\epsilon} ||^2_2, \\
\end{align*}
Show this is true by substituting your answer from the previous question into $\frac{1}{n}\|X\hat{\bw}-\bt|_2^2$. Also, find the expectation of the above training error in terms of $n, d$ and $\sigma$. \\

\noindent \emph{Hints: you might find the cyclic property \footnote{\url{https://en.wikipedia.org/wiki/Trace_(linear_algebra)\#Cyclic_property}} of trace useful. }

\subsection{Overparameterized Model}

\subsubsection{{\color{blue}[0.5pt]} \LI}
Now consider the overparameterized $d>n$ case. We first illustrate that there exist multiple empirical risk minimizers.
For simplicity we let $n=1$ and $d=2$.
Choose $\bx_1 = [1;1]$ and $t_1 = 3$, i.e. the one data point and all possible $\hat{\bw}$ lie on a 2D plane.
Show that there exists infinitely many $\hat{\bw}$ satisfying $\hat{\bw}^\top\bx_1=y_1$ on a real line.
Write down the equation of the line.

\subsubsection{{\color{blue}[0.5pt]} \LIII\EC}

Now, let's generalize the previous 2D case to the general $d>n$. Show that gradient descent from zero initialization i.e. $\hat{\bw}(0) = 0$ finds a unique minimizer if it converges. Show that the solution by gradient decent is $\hat{\bw} = X^\top (XX^\top)^{-1}\bt$. Show your work. \\

\noindent \emph{Hints: You can assume that the gradient is spanned by the rows of $X$ and write $\hat{\bw} = X^\top \mathbf{a}$ for some $\mathbf{a}\in\mathbb{R}^n$.}

\subsubsection{{[0pt]} \LIII} 
Repeat part \ref{sec:n_reg} for the overparameterized case.

\subsubsection{{\color{blue}[0.5pt]} \LIV}
Visualize and compare underparameterized with overparameterized polynomial regression:
\url{https://colab.research.google.com/github/uoft-csc413/2023/blob/master/assets/assignments/LS_polynomial_regression.ipynb}
Include your code snippets for the \verb|fit_poly| function in the write-up. Does overparameterization (higher degree polynomial) always lead to overfitting, i.e. larger test error?

\subsubsection{[0pt] \LIV}
Give $n_1$, $n_2$ with $n_1 \leq n_2$, and fixed dimension $d$ for which $L_2 \geq L_1$, i.e. the loss with $n_2$ data points is greater than loss with $n_1$ data points. Explain the underlying phenomenon. Be sure to also include the error values $L_1$ and $L_2$ or provide visualization in your solution. 

\noindent \emph{Hints: use your code to experiment with relevant parameters, then vary to find region and report one such setting.}



\section{Backpropagation (4pts)} 
This question helps you to understand the underlying mechanism of back-propagation. You need to have a clear understanding of what happens during the forward pass and backward pass and be able to reason about the time complexity and space complexity of your neural network. Moreover, you will learn a commonly used trick to compute the gradient norm efficiently without explicitly writing down the whole Jacobian matrix.\\

\noindent Note: The reading on backpropagation located at \url{https://uoft-csc413.github.io/2023/assets/readings/L02b.pdf} may be useful for this question.

\subsection{Automatic Differentiation}
Consider a neural network defined with the following procedure:
\begin{align*}
    \intermediates_1 &= \weightMatL{1} \inputVec + \biasesL{1} \\
    \hiddens_1 &= \textnormal{ReLU}(\intermediates_1) \\
    \intermediates_2 &= \weightMatL{2} \inputVec + \biasesL{2} \\
    \hiddens_2 &= \sigma(\intermediates_2) \\
    \mathbf{g} &= \hiddens_1 \circ \hiddens_2 \\
    \predictions &=  \weightMatL{3} \mathbf{g} + \weightMatL{4} \inputVec,\\
    \predictions' &= \textnormal{softmax}(\predictions)\\ 
    \mathcal{S} &= \sum_{k=1}^{N}\indicator(t = k)\log(\predictions'_{k})\\
    \cost &= -\mathcal{S}
\end{align*}
for input $\inputVec$ with class label $t$ where $\textnormal{ReLU}(\intermediates) = \max(\intermediates, 0)$ denotes the $\textnormal{ReLU}$ activation function, $\sigma(\intermediates) = \frac{1}{1+e^{-\intermediates}}$ denotes the Sigmoid activation function, both applied elementwise, and $\textnormal{softmax}(\predictions) = \frac{\exp(\predictions)}{\sum_{i = 1}^{N}\exp(\predictions_i)}$.
Here, $\circ$ denotes element-wise multiplication. 

\subsubsection{Computational Graph {[0pt]} \LIV}
    Draw the computation graph relating $\inputVec$, $t$, $\intermediates_1$, $\intermediates_2$, $\hiddens_1$, $\hiddens_2$ , $\mathbf{g}$, $\predictions$, $\predictions'$, $\mathcal{S}$ and $\cost$.

\subsubsection{Backward Pass {\color{blue}[1pt]} \LIII}
    Derive the backprop equations for computing $\Bar{\mathbf{x}} = {\frac{\partial \cost}{\partial \mathbf{x}}}^{\top}$, one variable at a time, similar to the vectorized backward pass derived in Lec 2. \\
    
    \noindent \emph{Hints: Be careful about the transpose and shape! Assume all vectors (including error vector) are column vector and all Jacobian matrices adopt numerator-layout notation \footnote{Numerator-layout notation: \url{https://en.wikipedia.org/wiki/Matrix_calculus\#Numerator-layout_notation}}. You can use $\text{softmax}^\prime(\predictions)$ for the Jacobian matrix of softmax.}

\subsection{Gradient Norm Computation}
Many deep learning algorithms require you to compute the $L^2$ norm of the gradient of a loss function with respect to the model parameters for every example in a minibatch. Unfortunately, most differentiation functionality provided by most software frameworks (Tensorflow, PyTorch) does not support computing gradients for individual samples in a minibatch. Instead, they only give one gradient per minibatch that aggregates individual gradients for you. A naive way to get the per-example gradient norm is to use a batch size of 1 and repeat the back-propagation $N$ times, where $N$ is the minibatch size. After that, you can compute the $L^2$ norm of each gradient vector. As you can imagine, this approach is very inefficient. It can not exploit the parallelism of minibatch operations provided by the framework. \\

In this question, we will investigate a more efficient way to compute the per-example gradient norm and reason about its complexity compared to the naive method. For simplicity, let us consider the following two-layer neural network.

\begin{align*}
    \intermediates &= \weightMatL{1} \inputVec \\
    \hiddens &= \textnormal{ReLU}(\intermediates) \\
    \predictions &= \weightMatL{2} \hiddens,\\
\end{align*}

\noindent where $\weightMatL{1} = \begin{pmatrix}
            1 & 2 & 1 \\
            -2 & 1 & 0 \\
            1 & -2 & -1 \\
            \end{pmatrix}$
and $\weightMatL{2} = \begin{pmatrix}
            -2 & 4 & 1\\
            1 & -2 & -3\\
            -3 & 4 & 6
            \end{pmatrix}$.

\subsubsection{Naive Computation {\color{blue}[1pt]} \LIII}
Let us assume the input $x=\begin{pmatrix} 1 & 3 & 1 \end{pmatrix}^{\top}$ and the error vector $\lossDeriv{\predictions}= \frac{\partial \cost}{\partial \predictions}^{\top} = {\begin{pmatrix} 1 & 1 & 1 \end{pmatrix}}^{\top}$. In this question, write down the Jacobian matrix (numerical value) $\frac{\partial \cost}{\partial \weightMatL{1}}$ and $\frac{\partial \cost}{\partial \weightMatL{2}}$ using back-propagation. Then, compute the square of Frobenius Norm of the two Jacobian matrices, $\|A\|^2_{\mathrm{F}}$. The square of Frobenius norm of a matrix $A$ is defined as follows:
$$
\|A\|^2_{\mathrm{F}}={\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}}={\operatorname{trace}\left(A^{\top} A\right)}
$$

\noindent \emph{Hints: Be careful about the transpose. Show all your work for partial marks.}


\subsubsection{Efficient Computation {\color{blue}[0.5pt]} \LIII}
Notice that weight Jacobian can be expressed as the outer product of the error vector and activation $\frac{\partial \cost}{\partial \weightMatL{1}} = (\lossDeriv{\intermediates} \inputVec^{\top})^{\top}$ and $\frac{\partial \cost}{\partial \weightMatL{2}} = (\lossDeriv{\predictions} \hiddens^{\top})^{\top}$. We can compute the Jacobian norm more efficiently using the following trick:

\begin{align*}
    \|\frac{\partial \cost}{\partial \weightMatL{1}}\|^2_{\mathrm{F}} &={\operatorname{trace}\left({\frac{\partial \cost}{\partial \weightMatL{1}}}^{\top} \frac{\partial \cost}{\partial \weightMatL{1}} \right)} \qquad (\text{Definition}) \\
    &= {\operatorname{trace}\left(\lossDeriv{\intermediates} \inputVec^{\top} \inputVec {\lossDeriv{\intermediates}}^{\top} \right)} \\
    &= {\operatorname{trace}\left({\inputVec}^{\top} \inputVec {\lossDeriv{\intermediates}}^{\top} \lossDeriv{\intermediates} \right)} \qquad (\text{Cyclic Property of Trace}) \\
    &= {\left({\inputVec}^{\top} \inputVec \right) \left({\lossDeriv{\intermediates}}^{\top} \lossDeriv{\intermediates} \right)} \qquad (\text{Scalar Multiplication})\\
    &= \|\inputVec\|^2_2 \|\lossDeriv{\intermediates}\|^2_2
\end{align*} \\

\noindent Compute the \textbf{square} of the Frobenius Norm of the two Jacobian matrices by plugging the value into the above trick. 

\noindent \emph{Hints: Verify the solution is the same as naive computation. Show all your work for partial marks.}


\subsubsection{Complexity Analysis {\color{blue}[1.5pt]} \LII}
Now, let us consider a general neural network with $K-1$ hidden layers ($K$ weight matrices). All input units, output units, and hidden units have a dimension of $D$. Assume we have $N$ input vectors. How many scalar multiplications $T$ (integer) do we need to compute the per-example gradient norm using naive and efficient computation, respectively? And, what is the memory cost $M$ (big $\mathcal{O}$ notation)? \\

\noindent For simplicity, you can ignore the activation function and loss function computation. You can assume the network does not have a bias term. You can also assume there are no in-place operations. Please fill up the table below. 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
                          & T (Naive) & T (Efficient) & M (Naive) & M (Efficient) \\ \hline
Forward Pass              &           &               &           &               \\ \hline
Backward Pass             &           &               &           &               \\ \hline
Gradient Norm Computation &           &               &           &               \\ \hline
\end{tabular}
\end{table}

\noindent \emph{Hints: The forward pass computes all the activations and needs memory to store model parameters and activations. The backward pass computes all the error vectors. Moreover, you also need to compute the parameter's gradient in naive computation. During the Gradient Norm Computation, the naive method needs to square the gradient before aggregation. In contrast, the efficient method relies on the trick. Thinking about the following questions may be helpful. 1) Do we need to store all activations in the forward pass? 2) Do we need to store all error vectors in the backward pass? 3) Why standard backward pass is twice more expensive than the forward pass? Don't forget to consider $K$ and $N$ in your answer.}


\subsection{Inner product of Jacobian: JVP and VJP {[0pt]} \LIII}
A more general case of computing the gradient norm is to compute the inner product of the Jacobian matrices computed using two different examples. Let $f_1, f_2$ and $y_1, y_2$ be the final outputs and layer outputs of two different examples respectively. The inner product $\Theta$ of Jacobian matrices of layer parameterized by $\theta$ is defined as:
$$
\Theta_{\theta}\left(f_{1}, f_{2}\right):=\frac{\partial f_{1}}{\partial \theta} \frac{\partial f_{2}}{\partial \theta}^{\top}=\frac{\partial f_{1}}{\partial y_{1}} \frac{\partial y_{1}}{\partial \theta} \frac{\partial y_{2}}{\partial \theta}^{\top} \frac{\partial f_{2}}{\partial y_{2}}^{\top}= \underbrace{\frac{\partial f_{1}}{\partial y_{1}}}_{\mathbf{O} \times \mathbf{Y}} \underbrace{\frac{\partial y_{1}}{\partial \theta}}_{\mathbf{Y} \times \mathbf{P}} \underbrace{\frac{\partial y_{2}}{\partial \theta}^{\top}}_{\mathbf{P} \times \mathbf{Y}} \underbrace{\frac{\partial f_{2}}{\partial y_{2}}^{\top}}_{\mathbf{Y} \times \mathbf{O}},
$$
Where $\mathbf{O}, \mathbf{Y}, \mathbf{P}$ represent the dimension of the final output, layer output, model parameter respectively. 
How to formulate the above computation using Jacobian Vector Product (JVP) and Vector Jacobian Product (VJP)? What are the computation cost using the following three ways of contracting the above equation? \\

\noindent (a) Outside-in: $M_1M_2M_3M_4 = ((M_1M_2)(M_3M_4))$  \\
(b) Left-to-right and right-to-left: $M_1M_2M_3M_4=(((M_1M_2)M_3)M_4)=(M_1(M_2(M_3M_4)))$  \\
(c) Inside-out-left and inside-out-right: $M_1M_2M_3M_4 = ((M_1(M_2M_3))M_4) = (M_1((M_2M_3)M_4))$     


\section{Hard-Coding Networks (2.5pts) \LIV}
Can we use neural networks to tackle coding problems? Yes! In this question, you will build a neural network to find the $k^{th}$ smallest number from a list using two different approaches: sorting and counting (Optional). You will start by constructing a two-layer perceptron ``Sort\_2" to sort two numbers and then use it as a building block to perform your favorite sorting algorithm (e.g., Bubble Sort, Merge Sort). Finally, you will output the $k^{th}$ element from the sorted list as the final answer. \\

\noindent Note: Before doing this problem, you need to have a basic understanding of the key components of neural networks (e.g., weights, activation functions). The reading on multilayer perceptrons located at \url{https://uoft-csc413.github.io/2023/assets/readings/L02a.pdf} may be useful. 

\subsection{Sort two numbers {\color{blue}[1pt]}}\label{sort_2}
    In this problem, you need to find a set of weights and bias for a two-layer perceptron ``Sort\_2" that sorts two numbers. The network takes a pair of numbers $(x_1, x_2)$ as input and output a sorted pair $(y_1, y_2)$, where $y_1 \leq y_2$. You may assume the two numbers are distinct and positive for simplicity. You will use the following architecture:
    \begin{center}
        \includegraphics[width=0.3 \textwidth]{a1/figures/q1.1.pdf}
    \end{center}
    
    \noindent Please specify the weights and activation functions for your network.
    Your answer should include: 
    \begin{itemize}
        \item Two weight matrices: $\weightMatL{1}$, $\weightMatL{2} \in \mathbb{R}^{2 \times 2}$
        \item Two bias vector: $\biasesL{1}$, $\biasesL{2} \in \mathbb{R}^{2}$ 
        \item Two activation functions: $\phi^{(1)}(z)$, $\phi^{(2)}(z)$
    \end{itemize}
    You do not need to show your work. \\
    
    \noindent \emph{Hints: Sorting two numbers is equivalent to finding the min and max of two numbers.} \\
    $$\max(x_1,x_2) = \frac{1}{2}(x_1+x_2)+\frac{1}{2}|x_1-x_2|,\qquad \min(x_1,x_2) = \frac{1}{2}(x_1+x_2)-\frac{1}{2}|x_1-x_2|$$

\subsection{Perform Sort {\color{blue}[1.5pt]}\EC}\label{sec:perform_sort}
    Draw a computation graph to show how to implement a sorting function $\hat{f}:\R^4 \to \R^4$ where $\hat{f}(x_1, x_2, x_3, x_4) = (\hat{x}_1, \hat{x}_2, \hat{x}_3, \hat{x}_4)$ where $(\hat{x}_1, \hat{x}_2, \hat{x}_3, \hat{x}_4)$ is $(x_1, x_2, x_3, x_4)$ in sorted order.
    Let us assume $\hat{x}_1 \leq \hat{x}_2 \leq \hat{x}_3 \leq \hat{x}_4$ and $x_1, x_2, x_3, x_4$ are positive and distinct. Implement $\hat{f}$ using your favourite sorting algorithms (e.g. Bubble Sort, Merge Sort). Let us denote the ``Sort\_2" module as $S$, please complete the following computation graph. Your answer does not need to give the label for intermediate nodes, but make sure to index the ``Sort\_2" module.
    
    \begin{center}
        \includegraphics[width=0.3 \textwidth]{a1/figures/q1.2.pdf}
    \end{center}
    
    \noindent \emph{Hints: Bubble Sort needs 6 ``Sort\_2" blocks, while Merge Sort needs 5 ``Sort\_2" blocks.}

\subsection{Find the $k^{th}$ smallest number {[0pt]}}
Based on your sorting network, you may want to add a new layer to output your final result ($k^{th}$ smallest number). Please give the weight $\weightMatL{3}$ for this output layer when $k=3$. \\

\noindent \emph{Hints: $\weightMatL{3} \in \mathbb{R}^{1 \times 4}$ .}

\subsection{Counting Network {[0pt]}}
The idea of using a counting network to find the $k^{th}$ smallest number is to build a neural network that can determine the rank of each number and output the number with the correct rank. 
Specifically, the counting network will count how many elements in a list are less than a value of interest. And you will apply the counting network to all numbers in the given list to determine their rank. Finally, you will use another layer to output the number with the correct rank.

\noindent The counting network has the following architecture, where $y$ is the rank of $x_1$ in a list containing $x_1, x_2, x_3, x_4$. 
    \begin{center}
        \includegraphics[width=0.3 \textwidth]{figures/architecture.png} 
    \end{center} 
    
\noindent Please specify the weights and activation functions for your counting network. Draw a diagram to show how you will use the counting network and give a set of weights and biases for the final layer to find the $k^{th}$ smallest number. In other words, repeat the process of sections 1.1, 1.2, 1.3 using the counting idea. \\

\noindent \emph{Hints: You may find the following two activation functions useful.} \\
\noindent \emph{1) Hard threshold activation function:}
    \[ \phi(z) = \indicator(z \geq 0) = \left\{ \begin{array}{ll} 1 & \textrm{if } z \geq 0 \\ 0 & \textrm{if } z < 0 \end{array} \right. \]
\emph{2) Indicator activation function:}
    \[ \phi(z) = \indicator(z \in [-1, 1]) = \left\{ \begin{array}{ll} 1 & \textrm{if } z \in [-1, 1] \\ 0 & \textrm{otherwise} \end{array} \right. \]


\section{Bias in Word Embeddings (2pts)}

Unfortunately, stereotypes and prejudices are often reflected in the outputs of natural language processing algorithms. For example, Google Translate is more likely to translate a non-English sentence to \say{\textit{He} is a doctor} than \say{\textit{She} is a doctor} when the sentence is ambiguous. In this section, you will explore how bias\footnote{In AI and machine learning, \textbf{bias} generally refers to prior information, a necessary prerequisite for intelligent action. However, bias can be problematic when it is derived from aspects of human culture known to lead to harmful behaviour, such as stereotypes and prejudices.} enters natural language processing algorithms by implementing and analyzing a popular method for measuring bias in word embeddings.

\subsection{WEAT method for detecting bias \textcolor{blue}{[1pt]} \LII}

Word embedding models such as GloVe attempt to learn a vector space where semantically similar words are clustered close together. However, they have been shown to learn problematic associations, e.g. by embedding \say{man} more closely to \say{doctor} than \say{woman} (and vice versa for \say{nurse}). To detect such biases in word embeddings, \cite{caliskan2017semantics} introduced the Word Embedding Association Test (WEAT). The WEAT test measures whether two \textit{target} word sets (e.g. \{programmer, engineer, scientist, ...\} and \{nurse, teacher, librarian, ...\}) have the same relative association to two \textit{attribute} word sets (e.g. \{man, male, ...\} and \{woman, female ...\}).\footnote{There is an excellent blog on bias in word embeddings and the WEAT test at \url{https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html}}

Formally, let  \(A\), \(B\) be two sets of attribute words. Then

\begin{align}
  s(w, A, B) = \text{mean}_{a \in A} \cos(\vec{w}, \vec{a}) - \text{mean}_{b \in B} \cos(\vec{w}, \vec{b})
\end{align}

measures the association of a target word \(w\) with the attribute sets - for convenience, we will call this the WEAT association score. A positive score means that the word $w$ is more associated with A, while a negative score means the opposite. For example, a WEAT association score of 1 in the following test $s(``programmer", \{man\}, \{woman\})=1$, implies the ``programmer'' has a stronger association to $\{man\}$. For reference, the cosine similarity between two word vectors \(\vec{a}\) and \(\vec{b}\) is given by:

\begin{align}
    \cos(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\parallel \vec{a} \parallel  \parallel \vec{b} \parallel}
\end{align}

In the notebook, we have provided example target words (in sets \(X\) and \(Y\)) and attribute words (in sets \(A\) and \(B\)). You must implement the function \verb+weat_association_score()+ and compute the WEAT association score for each target word. 

\subsection{Reasons for bias in word embeddings [0pt] \LII}

Based on the results of the WEAT test, do the pretrained word embeddings associate certain occuptations with one gender more than another? What might cause word embedding models to learn certain stereotypes and prejudices? How might this be a problem in downstream applications?

\subsection{Analyzing WEAT}

While WEAT makes intuitive sense by asserting that closeness in the embedding space indicates greater similarity, more recent work \cite{ethayarajh2019understanding} has further analyzed the mathematical assertions and found some drawbacks this method. Analyzing edge cases is a good way to find logical inconsistencies with any algorithm, and WEAT in particular can behave strangely when A and B contain just one word each. 
 
\subsubsection{1-word subsets \textcolor{blue}{[0.5pts]} \LIV} In the notebook, you are asked to find 1-word subsets of the original A and B that reverse the association between some of the occupations and the gendered attributes (change the sign of the WEAT score).

\subsubsection{How word frequency affects embedding similarity \textcolor{blue}{[0.5pts]} \LIII\EC}

Next, consider this fact about word embeddings, which has been verified empirically and theoretically: The squared norm of a word embedding is linear in the log probability of the word in the training corpus. In other words, the more common a word is in the training corpus, the larger the norm of its word embedding. Following this fact, we will show how one may exploit the WEAT association score for a specific word embedding model. Let us start with three word embedding vectors: a target word $\mathbf{w}_i$ and two attributes $\{\mathbf{w}_j\}$, $\{\mathbf{w}_k\}$. 
\begin{align*}
    s(\mathbf{w}_i, \{\mathbf{w}_j\}, \{\mathbf{w}_k\}) &= cos(\mathbf{w}_i, \mathbf{w}_j) - cos(\mathbf{w}_i, \mathbf{w}_k) \\
    &= \frac{\mathbf{w}_i^\top\mathbf{w}_j}{\|\mathbf{w}_i\|\|\mathbf{w}_j\|} - \frac{\mathbf{w}_i^\top\mathbf{w}_k}{\|\mathbf{w}_i\|\|\mathbf{w}_k\|}
\end{align*}
Remember the GloVe embedding training objective from Part 1 in this assignment. Assume tied weights and ignore the bias units, we can write the training loss as following:
\begin{gather*}
    \text{Simplified GloVe }\quad L(\{\mathbf{w}_i\}_{i=1}^V) = \sum_{i,j=1}^V (\mathbf{w}_i^\top{\mathbf{w}}_j - \log X_{ij})^2, 
\end{gather*}
where $X_{ij}$ denotes the number of times word $i$ and word $j$ co-occured together in the training corpus. In the special case, $X_{ii}$ denotes the number of times word $i$ appeared in the corpus. When this model reaches zero training loss, the inner product of the GloVe embedding vectors will simply equal to the entries in the log co-occurrence matrix $\log X$. We can then express the WEAT association score in terms of the original co-occurrence matrix:
\begin{gather*}
    s(\mathbf{w}_i, \{\mathbf{w}_j\}, \{\mathbf{w}_k\})= \frac{1}{\sqrt{logX_{ii}}}\left(\frac{{logX_{ij}}}{\sqrt{logX_{jj}}} - \frac{{logX_{ik}}}{\sqrt{logX_{kk}}}\right)
\end{gather*}
Briefly explain how this fact might contribute to the results from the previous section when using different attribute words. Provide your answers in no more than three sentences.

\textit{Hint: The paper cited above is a great resource if you are stuck.}

\subsubsection{Relative association between two sets of target words [0 pts] \LIII}
In the original WEAT paper, the authors do not examine the association of individual words with attributes, but rather compare the relative association of two sets of target words. For example, are insect words more associated with positive attributes or negative attributes than flower words.

Formally, let \(X\) and \(Y\) be two sets of target words of equal size. The WEAT test statistic is given by:
\begin{align}
    s(X, Y, A, B) = \sum_{x\in X} s(x, A, B) - \sum_{y \in Y} s(y, A, B)
\end{align}

Will the same technique from the previous section work to manipulate this test statistic as well? Provide your answer in no more than 3 sentences.